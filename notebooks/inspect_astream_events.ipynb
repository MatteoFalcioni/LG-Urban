{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting `astream_events()`\n",
    "\n",
    "LangGraph `astream_events()` function is very useful to filter out events during streaming, but its output is super nested, and seems to even change between Claude and GPT from Claude version 4.5. \n",
    "\n",
    "Let's inspect that, and let's also inspect how we can stream reasoning tokens from the models while they think, while only outputting the last response at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded\n"
     ]
    }
   ],
   "source": [
    "# Inspect all event types and data (mimicking api.py behavior)\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy tool created\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy tool that returns mock artifacts\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "from typing_extensions import Annotated\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "\n",
    "@tool\n",
    "def dummy_tool(\n",
    "    query: Annotated[str, \"A query string\"],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"A dummy tool that returns mock artifacts\"\"\"\n",
    "    \n",
    "    # Simulate artifacts like code_sandbox returns\n",
    "    mock_artifacts = [\n",
    "        {\n",
    "            \"name\": \"test_image.png\",\n",
    "            \"mime\": \"image/png\",\n",
    "            \"url\": \"/api/artifacts/123?token=abc\",\n",
    "            \"size\": 12345\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"data.csv\",\n",
    "            \"mime\": \"text/csv\",\n",
    "            \"url\": \"/api/artifacts/456?token=def\",\n",
    "            \"size\": 67890\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Return ToolMessage wrapped in Command (like code_sandbox does)\n",
    "    tool_msg = ToolMessage(\n",
    "        content=f\"Processed query: {query}\",\n",
    "        artifact=mock_artifacts,\n",
    "        tool_call_id=tool_call_id\n",
    "    )\n",
    "    \n",
    "    return Command(update={\"messages\": [tool_msg]})\n",
    "\n",
    "print(\"Dummy tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created with dummy tool\n"
     ]
    }
   ],
   "source": [
    "# Create a simple graph with just the dummy tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Create LLM\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-5\",\n",
    "    temperature=1,\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"reasoning\": {\n",
    "            \"effort\": \"low\",\n",
    "            \"summary\": \"auto\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-sonnet-4-5\")\n",
    "\n",
    "# Create agent with just the dummy tool\n",
    "memory = MemorySaver()\n",
    "graph = create_react_agent(openai_llm, tools=[dummy_tool], checkpointer=memory)\n",
    "\n",
    "print(\"Graph created with dummy tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STREAMING ALL EVENTS (mimicking api.py)\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AsyncCompletions.create() got an unexpected keyword argument 'reasoning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     10\u001b[39m event_count = {}\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream_events(state, config, version=\u001b[33m\"\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     13\u001b[39m     event_type = event.get(\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     name = event.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:1540\u001b[39m, in \u001b[36mRunnable.astream_events\u001b[39m\u001b[34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1539\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[32m   1541\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:1034\u001b[39m, in \u001b[36m_astream_events_implementation_v2\u001b[39m\u001b[34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(asyncio.CancelledError):\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:989\u001b[39m, in \u001b[36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable.astream(value, config, **kwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer.tap_output_aiter(run_id, stream):\n\u001b[32m    990\u001b[39m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[32m    991\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:190\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    188\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/utils/aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/pregel/_retry.py:132\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m task.proc.astream(task.input, config):\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:840\u001b[39m, in \u001b[36mRunnableSeq.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m             aiterator = h.tap_output_aiter(\n\u001b[32m    837\u001b[39m                 run_manager.run_id, aiterator\n\u001b[32m    838\u001b[39m             )\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# consume into final output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    841\u001b[39m     _consume_aiter(aiterator), context=context\n\u001b[32m    842\u001b[39m )\n\u001b[32m    843\u001b[39m \u001b[38;5;66;03m# sequence doesn't emit output, yield to mark as generator\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:905\u001b[39m, in \u001b[36m_consume_aiter\u001b[39m\u001b[34m(it)\u001b[39m\n\u001b[32m    903\u001b[39m output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    904\u001b[39m add_supported = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# collect final output\u001b[39;00m\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m add_supported:\n\u001b[32m    908\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:190\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    188\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/utils/aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:1613\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1610\u001b[39m final: Input\n\u001b[32m   1611\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1613\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1614\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1615\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1622\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:1613\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1610\u001b[39m final: Input\n\u001b[32m   1611\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1613\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1614\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1615\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1622\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:1176\u001b[39m, in \u001b[36mRunnable.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mastream\u001b[39m(\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   1160\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1161\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   1162\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m   1163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default implementation of ``astream``, which calls ``ainvoke``.\u001b[39;00m\n\u001b[32m   1164\u001b[39m \n\u001b[32m   1165\u001b[39m \u001b[33;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \n\u001b[32m   1175\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:465\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m         run = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m         ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    467\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:655\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    653\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m dynamic_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m static_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    658\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:3290\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3288\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3289\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3290\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3291\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:5723\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5716\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5717\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5721\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5722\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5724\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5725\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5726\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5727\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:417\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Any,\n\u001b[32m    415\u001b[39m ) -> BaseMessage:\n\u001b[32m    416\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    418\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    419\u001b[39m         stop=stop,\n\u001b[32m    420\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    421\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    422\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    423\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    424\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    425\u001b[39m         **kwargs,\n\u001b[32m    426\u001b[39m     )\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1034\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     **kwargs: Any,\n\u001b[32m   1032\u001b[39m ) -> LLMResult:\n\u001b[32m   1033\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1035\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1036\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:992\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    979\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    980\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    981\u001b[39m             *[\n\u001b[32m    982\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m             ]\n\u001b[32m    991\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    993\u001b[39m flattened_outputs = [\n\u001b[32m    994\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    996\u001b[39m ]\n\u001b[32m    997\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1151\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream(\n\u001b[32m   1146\u001b[39m     async_api=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1147\u001b[39m     run_manager=run_manager,\n\u001b[32m   1148\u001b[39m     **kwargs,\n\u001b[32m   1149\u001b[39m ):\n\u001b[32m   1150\u001b[39m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] = []\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._astream(messages, stop=stop, **kwargs):\n\u001b[32m   1152\u001b[39m         chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2015\u001b[39m, in \u001b[36mChatOpenAI._astream\u001b[39m\u001b[34m(self, stream_usage, *args, **kwargs)\u001b[39m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_usage:\n\u001b[32m   2013\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: stream_usage}\n\u001b[32m-> \u001b[39m\u001b[32m2015\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._astream(*args, **kwargs):\n\u001b[32m   2016\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:785\u001b[39m, in \u001b[36mBaseChatOpenAI._astream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    783\u001b[39m     base_generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m response:\n\u001b[32m    787\u001b[39m     is_first_chunk = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langgraph-py3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: AsyncCompletions.create() got an unexpected keyword argument 'reasoning'",
      "During task with name 'agent' and id '55d4545c-04b6-db4d-7df4-398c284c3168'"
     ]
    }
   ],
   "source": [
    "# Stream ALL events and show what would be sent to frontend\n",
    "user_text = \"Think for a while, talk a little bit to yourself, then use the dummy tool to search for 'test query'\"\n",
    "state = {\"messages\": [{\"role\": \"user\", \"content\": user_text}]}\n",
    "config = {\"configurable\": {\"thread_id\": \"test-123\"}}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STREAMING ALL EVENTS (mimicking api.py)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "event_count = {}\n",
    "\n",
    "async for event in graph.astream_events(state, config, version=\"v2\"):\n",
    "    event_type = event.get(\"event\")\n",
    "    name = event.get(\"name\")\n",
    "    data = event.get(\"data\", {})\n",
    "    metadata = event.get(\"metadata\", {})\n",
    "    \n",
    "    # Count event types\n",
    "    event_count[event_type] = event_count.get(event_type, 0) + 1\n",
    "    \n",
    "    # Filter to relevant events (like api.py does)\n",
    "    if event_type in [\"on_chat_model_stream\", \"on_tool_start\", \"on_tool_end\", \"on_chat_model_end\"]:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVENT: {event_type}\")\n",
    "        print(f\"NAME: {name}\")\n",
    "        print(f\"METADATA: {metadata}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if event_type == \"on_chat_model_stream\":\n",
    "            # Show what chunk would be sent to frontend\n",
    "            chunk = data.get(\"chunk\")\n",
    "            if chunk:\n",
    "                content = getattr(chunk, \"content\", \"\")\n",
    "                print(f\"CHUNK CONTENT: {repr(content)}\")\n",
    "                \n",
    "                # Check for tool calls\n",
    "                if hasattr(chunk, \"tool_call_chunks\") and chunk.tool_call_chunks:\n",
    "                    print(f\"TOOL CALL CHUNKS: {chunk.tool_call_chunks}\")\n",
    "\n",
    "        elif event_type == \"on_chat_model_end\":\n",
    "            print(f\"CHAT MODEL END: {data}\")\n",
    "        \n",
    "        elif event_type == \"on_tool_start\":\n",
    "            # Show tool input\n",
    "            tool_input = data.get(\"input\")\n",
    "            print(f\"TOOL INPUT:\")\n",
    "            pprint(tool_input, indent=2)\n",
    "            \n",
    "            # This is what would be sent to frontend as SSE\n",
    "            sse_data = {\n",
    "                \"event\": \"tool_start\",\n",
    "                \"tool_name\": name,\n",
    "                \"tool_input\": tool_input\n",
    "            }\n",
    "            print(f\"\\nSSE TO FRONTEND:\")\n",
    "            print(json.dumps(sse_data, indent=2))\n",
    "        \n",
    "        elif event_type == \"on_tool_end\":\n",
    "            # Show raw tool output\n",
    "            raw_output = data.get(\"output\")\n",
    "            print(f\"RAW OUTPUT TYPE: {type(raw_output).__name__}\")\n",
    "            print(f\"RAW OUTPUT:\")\n",
    "            pprint(raw_output)\n",
    "            \n",
    "            # Extract ToolMessage (like api.py does)\n",
    "            artifacts = None\n",
    "            tool_call_id = None\n",
    "            content = None\n",
    "            \n",
    "            if hasattr(raw_output, \"update\") and isinstance(raw_output.update, dict):\n",
    "                msgs = raw_output.update.get(\"messages\", [])\n",
    "                if msgs:\n",
    "                    tm = msgs[0]\n",
    "                    tool_call_id = getattr(tm, \"tool_call_id\", None)\n",
    "                    content = getattr(tm, \"content\", None)\n",
    "                    artifacts = getattr(tm, \"artifact\", None)\n",
    "            \n",
    "            print(f\"\\nEXTRACTED:\")\n",
    "            print(f\"  tool_call_id: {tool_call_id}\")\n",
    "            print(f\"  content: {content}\")\n",
    "            print(f\"  artifacts:\")\n",
    "            pprint(artifacts, indent=4)\n",
    "            \n",
    "            # This is what would be sent to frontend as SSE\n",
    "            sse_data = {\n",
    "                \"event\": \"tool_end\",\n",
    "                \"tool_name\": name,\n",
    "                \"tool_call_id\": tool_call_id,\n",
    "                \"content\": content,\n",
    "                \"artifacts\": artifacts\n",
    "            }\n",
    "            print(f\"\\nSSE TO FRONTEND:\")\n",
    "            print(json.dumps(sse_data, indent=2, default=str))\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"EVENT SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "for event_type, count in sorted(event_count.items()):\n",
    "    print(f\"{event_type}: {count}\")\n",
    "print(f\"\\nTotal events: {sum(event_count.values())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
